#!/usr/bin/env python3
"""
Ejemplo de Generaci√≥n de Texto con Hugging Face
===============================================

Este script demuestra diferentes t√©cnicas de generaci√≥n de texto
usando modelos preentrenados de Hugging Face.
"""

from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, set_seed
import torch
import time
from typing import List, Dict

def generacion_basica():
    """
    Ejemplo b√°sico de generaci√≥n de texto usando pipeline
    """
    print("üöÄ Iniciando generaci√≥n b√°sica de texto...")
    
    # Configurar semilla para reproducibilidad
    set_seed(42)
    
    # Crear pipeline de generaci√≥n de texto
    # Usando GPT-2 en espa√±ol
    generator = pipeline(
        "text-generation",
        model="datificate/gpt2-small-spanish",
        tokenizer="datificate/gpt2-small-spanish",
        device=0 if torch.cuda.is_available() else -1
    )
    
    # Prompts de ejemplo
    prompts = [
        "En un futuro no muy lejano, la inteligencia artificial",
        "La receta secreta de la abuela inclu√≠a",
        "El explorador encontr√≥ en la cueva antigua",
        "La ciudad del futuro tendr√°",
        "El cient√≠fico descubri√≥ que"
    ]
    
    print("\nüìù Generando textos a partir de prompts:")
    print("-" * 60)
    
    resultados = []
    
    for i, prompt in enumerate(prompts, 1):
        print(f"\n{i}. Prompt: '{prompt}'")
        print("   Generando...")
        
        # Generar texto
        resultado = generator(
            prompt,
            max_length=100,
            num_return_sequences=2,
            temperature=0.8,
            do_sample=True,
            pad_token_id=generator.tokenizer.eos_token_id
        )
        
        for j, generacion in enumerate(resultado, 1):
            texto_generado = generacion['generated_text']
            texto_nuevo = texto_generado[len(prompt):].strip()
            
            print(f"   Opci√≥n {j}: {prompt}{texto_nuevo}")
            
            resultados.append({
                'prompt': prompt,
                'texto_generado': texto_generado,
                'texto_nuevo': texto_nuevo
            })
        
        print()
    
    return resultados

def generacion_avanzada_con_parametros():
    """
    Ejemplo avanzado mostrando diferentes par√°metros de generaci√≥n
    """
    print("üî¨ Generaci√≥n avanzada con diferentes par√°metros...")
    
    try:
        # Cargar modelo y tokenizer espec√≠ficos
        model_name = "microsoft/DialoGPT-medium"
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForCausalLM.from_pretrained(model_name)
        
        # A√±adir pad_token si no existe
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        generator = pipeline(
            "text-generation",
            model=model,
            tokenizer=tokenizer,
            device=0 if torch.cuda.is_available() else -1
        )
        
        prompt = "The future of artificial intelligence is"
        
        # Diferentes configuraciones de generaci√≥n
        configuraciones = [
            {
                'nombre': 'Creativo (alta temperatura)',
                'params': {'temperature': 1.2, 'top_p': 0.9, 'do_sample': True}
            },
            {
                'nombre': 'Conservador (baja temperatura)',
                'params': {'temperature': 0.3, 'top_p': 0.8, 'do_sample': True}
            },
            {
                'nombre': 'Determin√≠stico (greedy)',
                'params': {'do_sample': False, 'num_beams': 1}
            },
            {
                'nombre': 'Beam Search',
                'params': {'do_sample': False, 'num_beams': 4, 'early_stopping': True}
            }
        ]
        
        print(f"\nüìä Comparando diferentes estrategias con prompt: '{prompt}'")
        print("-" * 80)
        
        for config in configuraciones:
            print(f"\nüéØ {config['nombre']}:")
            
            params = {
                'max_length': 80,
                'num_return_sequences': 1,
                'pad_token_id': tokenizer.eos_token_id,
                **config['params']
            }
            
            start_time = time.time()
            resultado = generator(prompt, **params)
            end_time = time.time()
            
            texto_generado = resultado[0]['generated_text']
            tiempo = end_time - start_time
            
            print(f"   Resultado: {texto_generado}")
            print(f"   Tiempo: {tiempo:.2f}s")
            print()
            
    except Exception as e:
        print(f"‚ö†Ô∏è  Error con modelo avanzado: {e}")
        print("Continuando con modelo b√°sico...")

def generacion_conversacional():
    """
    Ejemplo de generaci√≥n conversacional/di√°logo
    """
    print("üí¨ Generaci√≥n conversacional...")
    
    try:
        # Usar modelo conversacional
        chatbot = pipeline(
            "conversational",
            model="microsoft/DialoGPT-medium",
            device=0 if torch.cuda.is_available() else -1
        )
        
        print("\nü§ñ Simulando conversaci√≥n:")
        print("-" * 40)
        
        # Simular una conversaci√≥n
        conversaciones = [
            "Hello! How are you today?",
            "What's your favorite programming language?",
            "Can you tell me a joke?",
            "What do you think about artificial intelligence?"
        ]
        
        from transformers import Conversation
        
        conversacion = Conversation()
        
        for i, mensaje in enumerate(conversaciones, 1):
            conversacion.add_user_input(mensaje)
            resultado = chatbot(conversacion)
            
            respuesta = resultado.generated_responses[-1]
            
            print(f"üë§ Usuario: {mensaje}")
            print(f"ü§ñ Bot: {respuesta}")
            print()
            
    except Exception as e:
        print(f"‚ö†Ô∏è  Error en generaci√≥n conversacional: {e}")

def generacion_con_control_de_estilo():
    """
    Ejemplo de generaci√≥n con control de estilo y formato
    """
    print("üé® Generaci√≥n con control de estilo...")
    
    # Usar modelo que permite mejor control
    generator = pipeline(
        "text-generation",
        model="gpt2",
        device=0 if torch.cuda.is_available() else -1
    )
    
    # Prompts con diferentes estilos
    estilos = [
        {
            'estilo': 'Cient√≠fico',
            'prompt': "According to recent research in neuroscience,",
            'params': {'temperature': 0.6, 'top_p': 0.9}
        },
        {
            'estilo': 'Narrativo',
            'prompt': "Once upon a time, in a magical forest,",
            'params': {'temperature': 1.0, 'top_p': 0.95}
        },
        {
            'estilo': 'T√©cnico',
            'prompt': "To implement this algorithm, we need to",
            'params': {'temperature': 0.4, 'top_p': 0.8}
        },
        {
            'estilo': 'Po√©tico',
            'prompt': "The moonlight dances on the water,",
            'params': {'temperature': 1.2, 'top_p': 0.9}
        }
    ]
    
    print("\nüé≠ Generando textos con diferentes estilos:")
    print("-" * 60)
    
    for estilo_info in estilos:
        print(f"\nüìù Estilo: {estilo_info['estilo']}")
        print(f"Prompt: '{estilo_info['prompt']}'")
        
        resultado = generator(
            estilo_info['prompt'],
            max_length=120,
            num_return_sequences=1,
            do_sample=True,
            pad_token_id=generator.tokenizer.eos_token_id,
            **estilo_info['params']
        )
        
        texto_completo = resultado[0]['generated_text']
        print(f"Resultado: {texto_completo}")
        print()

def ejemplo_interactivo_generacion():
    """
    Modo interactivo para que el usuario genere sus propios textos
    """
    print("\nüéÆ Modo Interactivo - Generaci√≥n de Texto")
    print("Ingresa un prompt y genera texto personalizado")
    print("Escribe 'salir' para terminar")
    print("-" * 50)
    
    # Inicializar generador
    generator = pipeline(
        "text-generation",
        model="gpt2",
        device=0 if torch.cuda.is_available() else -1
    )
    
    while True:
        prompt = input("\nüìù Ingresa tu prompt: ").strip()
        
        if prompt.lower() in ['salir', 'exit', 'quit', '']:
            print("üëã ¬°Hasta luego!")
            break
        
        # Configuraci√≥n personalizable
        print("\n‚öôÔ∏è  Configuraci√≥n (presiona Enter para usar valores por defecto):")
        
        try:
            max_length = input("Longitud m√°xima (50-200, default=100): ").strip()
            max_length = int(max_length) if max_length else 100
            max_length = max(50, min(200, max_length))  # Limitar rango
            
            temperature = input("Creatividad/Temperatura (0.1-2.0, default=0.8): ").strip()
            temperature = float(temperature) if temperature else 0.8
            temperature = max(0.1, min(2.0, temperature))  # Limitar rango
            
            num_sequences = input("N√∫mero de variaciones (1-3, default=1): ").strip()
            num_sequences = int(num_sequences) if num_sequences else 1
            num_sequences = max(1, min(3, num_sequences))  # Limitar rango
            
        except ValueError:
            print("‚ö†Ô∏è  Usando valores por defecto...")
            max_length, temperature, num_sequences = 100, 0.8, 1
        
        print(f"\nüîÑ Generando texto... (longitud: {max_length}, creatividad: {temperature})")
        
        try:
            start_time = time.time()
            resultados = generator(
                prompt,
                max_length=max_length,
                num_return_sequences=num_sequences,
                temperature=temperature,
                do_sample=True,
                pad_token_id=generator.tokenizer.eos_token_id,
                top_p=0.9
            )
            end_time = time.time()
            
            print(f"\n‚ú® Resultados generados en {end_time - start_time:.2f}s:")
            print("=" * 60)
            
            for i, resultado in enumerate(resultados, 1):
                texto_generado = resultado['generated_text']
                print(f"\n{i}. {texto_generado}")
            
            print("=" * 60)
            
        except Exception as e:
            print(f"‚ùå Error al generar texto: {e}")

def main():
    """
    Funci√≥n principal que ejecuta todos los ejemplos
    """
    print("ü§ó Ejemplos de Generaci√≥n de Texto con Hugging Face")
    print("=" * 60)
    
    try:
        # Ejemplo b√°sico
        print("1Ô∏è‚É£  Ejecutando generaci√≥n b√°sica...")
        generacion_basica()
        
        # Ejemplo avanzado
        print("\n2Ô∏è‚É£  Ejecutando generaci√≥n avanzada...")
        generacion_avanzada_con_parametros()
        
        # Generaci√≥n conversacional
        print("\n3Ô∏è‚É£  Ejecutando generaci√≥n conversacional...")
        generacion_conversacional()
        
        # Control de estilo
        print("\n4Ô∏è‚É£  Ejecutando control de estilo...")
        generacion_con_control_de_estilo()
        
        # Modo interactivo
        respuesta = input("\n¬øQuieres probar el modo interactivo? (s/n): ").lower()
        if respuesta in ['s', 'si', 's√≠', 'yes', 'y']:
            ejemplo_interactivo_generacion()
        
        print("\n‚úÖ ¬°Ejemplos de generaci√≥n completados exitosamente!")
        
        # Consejos finales
        print("\nüí° Consejos para mejorar la generaci√≥n:")
        print("   ‚Ä¢ Usa prompts m√°s espec√≠ficos para mejores resultados")
        print("   ‚Ä¢ Ajusta la temperatura: baja (0.3) = conservador, alta (1.2) = creativo")
        print("   ‚Ä¢ Experimenta con top_p y top_k para controlar la diversidad")
        print("   ‚Ä¢ Considera usar modelos espec√≠ficos para tu dominio")
        
    except Exception as e:
        print(f"‚ùå Error durante la ejecuci√≥n: {e}")
        print("üí° Aseg√∫rate de haber instalado todas las dependencias con:")
        print("   pip install -r requirements.txt")

if __name__ == "__main__":
    main()
